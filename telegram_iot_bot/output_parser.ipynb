{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI , ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator , field_validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory , ConversationBufferWindowMemory\n",
    "from langchain import LLMChain, PromptTemplate , ConversationChain\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "component_name_schema = ResponseSchema(name=\"component_name\",\n",
    "                             description=\"This is the name of the component or device\")\n",
    "\n",
    "reasoning_schema = ResponseSchema(name=\"reasoning\",\n",
    "                                      description=\"Just reasoning\")\n",
    "\n",
    "component_status_schema = ResponseSchema(name=\"component_status\",\n",
    "                                    description=\"This is an integer score between 0-9\")\n",
    "\n",
    "response_schemas = [component_name_schema, \n",
    "                    reasoning_schema,\n",
    "                    component_status_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "Select the tools as best as you can. You can also ignore if not possible. \n",
    "\n",
    "{chat_history}\n",
    "Examples : \n",
    "                                      \n",
    "Question: Switch On the light1\n",
    "component_name: light1\n",
    "reasoning: we have to switch it on\n",
    "component_status: 9                                  \n",
    "\n",
    "Question: Switch off the light1\n",
    "component_name: light1\n",
    "reasoning: we have to switch it off\n",
    "component_status: 0\n",
    "\n",
    "Question: Increase the fa1 speed to 5\n",
    "component_name: fa1\n",
    "reasoning: we have to set the level to 5\n",
    "component_status: 5       \n",
    "\n",
    "Question: Slow the fa2 speed to 2\n",
    "component_name: fa2\n",
    "reasoning: we have to set the level to 2\n",
    "component_status: 2  \n",
    "\n",
    "                                                          \n",
    "Read the Examples and answer Question1 using the following format for the output:\n",
    "{format_instructions}                                                         \n",
    "                    \n",
    "Begin!\n",
    "        \n",
    "Question1 : {question}\n",
    "                             \n",
    "\"\"\")\n",
    "\n",
    "# template = \"\"\"You are a teacher in physics for High School student. Given the text of question, it is your job to write a answer that question with example.\n",
    "# {chat_history}\n",
    "# Human: {question}\n",
    "# AI:\n",
    "# \"\"\"\n",
    "# prompt_template = PromptTemplate(input_variables=[\"chat_history\",'format_instructions',\"question\"], template=prompt)\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\" , k=1)\n",
    "\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "# llm_chain = LLMChain(\n",
    "#     llm=OpenAI(),\n",
    "#     prompt=prompt_template,\n",
    "#     verbose=True,\n",
    "#     memory=memory,\n",
    "    \n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"component_name\": string  // This is the name of the component or device\\n\\t\"reasoning\": string  // Just reasoning\\n\\t\"component_status\": string  // This is an integer score between 0-9\\n}\\n```'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n\\t\"component_name\": \"light1\",\\n\\t\"reasoning\": \"we have to dim it\",\\n\\t\"component_status\": \"5\"\\n}\\n```'\n",
      "{'component_name': 'light1', 'reasoning': 'we have to dim it', 'component_status': '5'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = chain.invoke({'question':\"dim the light1\" , 'format_instructions':format_instructions , 'chat_history':memory })\n",
    "\n",
    "print(response)\n",
    "# print(data)\n",
    "# print(response.content)\n",
    "\n",
    "response_as_dict = output_parser.parse(response.content)\n",
    "print(response_as_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
