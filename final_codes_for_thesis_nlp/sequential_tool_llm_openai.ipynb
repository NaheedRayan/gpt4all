{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (0.1.9)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (2.5.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (0.0.24)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (0.1.27)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (0.1.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain) (3.7.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: accelerate in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from accelerate) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: pyyaml in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from accelerate) (2.1.1)\n",
      "Requirement already satisfied: psutil in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: typing-extensions in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: jinja2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: filelock in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: networkx in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (0.41.2.post2)\n",
      "Requirement already satisfied: gpt4all in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (2.2.1.post1)\n",
      "Requirement already satisfied: requests in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from gpt4all) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from gpt4all) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->gpt4all) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->gpt4all) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->gpt4all) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->gpt4all) (2.1.0)\n",
      "Requirement already satisfied: transformers in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: filelock in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: requests in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: numexpr in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (2.8.7)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from numexpr) (1.26.4)\n",
      "Requirement already satisfied: google-search-results in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->google-search-results) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->google-search-results) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests->google-search-results) (3.6)\n",
      "Requirement already satisfied: langchain in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (0.1.9)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (0.0.24)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (0.1.27)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain) (0.1.9)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain) (3.7.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain\n",
    "!pip3 install accelerate\n",
    "!pip3 install bitsandbytes\n",
    "!pip install gpt4all \n",
    "!pip install transformers\n",
    "\n",
    "!pip install numexpr\n",
    "!pip install google-search-results\n",
    "\n",
    "!pip install langchain --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# from langchain.llms import GPT4All\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "# from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "\n",
    "\n",
    "from langchain.tools import Tool\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "# from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# from langchain.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI , ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"llmdataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df\n",
    "\n",
    "API_KEY = \"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\"\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding local path for general llm and for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = (\"/home/naheed/Desktop/gpt4all/model/mistral-7b-openorca.Q4_0.gguf\") #gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "# llm = GPT4All(model=local_path, device='cpu' ,callbacks=callbacks, verbose=True , streaming=True)\n",
    "llm = ChatOpenAI(temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiating model for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_promt = \"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "Search: useful for when you need to answer questions about current news\n",
    "Calculator: useful for when you need to answer questions about math\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [Search, Calculator]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SERPAPI_API_KEY=\"e1f8cd9de27d3b78f9ab3428167883002e76c33b\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "%env SERPAPI_API_KEY = \"e1f8cd9de27d3b78f9ab3428167883002e76c33b\"\n",
    "os.environ[\"SERPER_API_KEY\"] = \"e1f8cd9de27d3b78f9ab3428167883002e76c33b\"\n",
    "\n",
    "\n",
    "search = GoogleSerperAPIWrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naheed/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain/chains/llm_math/base.py:57: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "search = GoogleSerperAPIWrapper()\n",
    "ddg_search = DuckDuckGoSearchRun()\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
    "\n",
    "google_search = Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Google Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "        # description=\"useful for when you need to answer questions about current news\",\n",
    "        # coroutine= ... <- you can specify an async method if desired as well\n",
    "    )\n",
    "\n",
    "duckduckgo_search = Tool.from_function(\n",
    "        func=ddg_search.run,\n",
    "        name=\"DuckDuckGo Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "        # description=\"useful for when you need to answer questions about current news\",\n",
    "        # coroutine= ... <- you can specify an async method if desired as well\n",
    "    )\n",
    "\n",
    "wikipedia_search = Tool.from_function(\n",
    "        func=wikipedia.run,\n",
    "        name=\"Wiki Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "        # description=\"useful for when you need to answer questions about current news\",\n",
    "        # coroutine= ... <- you can specify an async method if desired as well\n",
    "    )\n",
    "\n",
    "calculator =  Tool.from_function(\n",
    "        func=llm_math_chain.run,\n",
    "        name=\"Calculator\",\n",
    "        description=\"useful for when you need to answer questions about math\",\n",
    "        # coroutine= ... <- you can specify an async method if desired as well\n",
    "    )\n",
    "\n",
    "# tools = [ google_search ,duckduckgo_search,wikipedia_search, calculator ]\n",
    "tools = [ google_search ,duckduckgo_search,calculator ]\n",
    "\n",
    "\n",
    "\n",
    "# # conversational agent memory\n",
    "# memory = ConversationBufferWindowMemory(\n",
    "#     memory_key='chat_history',\n",
    "#     k=3,\n",
    "#     return_messages=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template_asking_with_tools = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin! \n",
    "\n",
    "Question: {question}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "template_for_comapring_answers = \"\"\"\n",
    "\n",
    "Read the following Prompt1 and Prompt2 and determine if Prompt2 is available in Prompt1\n",
    "\n",
    "\n",
    "\n",
    "Prompt1: \"{llm_generated}\"\n",
    "\n",
    "Prompt2: \"{actual_answer}\"\n",
    "\n",
    "Options:\n",
    "\n",
    "Yes, the answer is available in the Prompt1 .\n",
    "No, the answer is not available in the Prompt2 sentence.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt1_with_tools = PromptTemplate(\n",
    "    template=template_asking_with_tools,\n",
    "    tools=tools,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "prompt2_for_comparison = PromptTemplate(\n",
    "    template=template_for_comapring_answers,\n",
    "    input_variables=[\"llm_generated\" , \"actual_answer\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Offline</th>\n",
       "      <th>Offline_time</th>\n",
       "      <th>Offline_error</th>\n",
       "      <th>Sequential_online</th>\n",
       "      <th>Sequential_online_time</th>\n",
       "      <th>Sequential_online_error</th>\n",
       "      <th>Merged_online</th>\n",
       "      <th>Merged_online_time</th>\n",
       "      <th>Merged_online_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who got Nobel Prize 2023 in Peace ?</td>\n",
       "      <td>Narges Mohammadi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who got The Nobel Prize in Physiology or Medic...</td>\n",
       "      <td>Drew Weissman &amp; Katalin Karikó</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who won 2023 Cricket World Cup?</td>\n",
       "      <td>Australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is the player of the series in 2023 Cricke...</td>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many teams are their in 2023 Cricket World...</td>\n",
       "      <td>10 teams</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>What is the number of bits in a byte?</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is the number of pixels in a 1080p resolu...</td>\n",
       "      <td>1920 x 1080 = 2073600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Who is the lead guitarist of the band Led Zepp...</td>\n",
       "      <td>Jimmy Page</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What is the number of atoms in a molecule of w...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>What is the number of sides of a circle?</td>\n",
       "      <td>A circle does not have any sides</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "0                 who got Nobel Prize 2023 in Peace ?   \n",
       "1   who got The Nobel Prize in Physiology or Medic...   \n",
       "2                     who won 2023 Cricket World Cup?   \n",
       "3   Who is the player of the series in 2023 Cricke...   \n",
       "4   How many teams are their in 2023 Cricket World...   \n",
       "..                                                ...   \n",
       "94              What is the number of bits in a byte?   \n",
       "95  What is the number of pixels in a 1080p resolu...   \n",
       "96  Who is the lead guitarist of the band Led Zepp...   \n",
       "97  What is the number of atoms in a molecule of w...   \n",
       "98           What is the number of sides of a circle?   \n",
       "\n",
       "                              Answer  Offline  Offline_time  Offline_error  \\\n",
       "0                   Narges Mohammadi      0.0           NaN            1.0   \n",
       "1     Drew Weissman & Katalin Karikó      0.0           NaN            NaN   \n",
       "2                          Australia      NaN           NaN            NaN   \n",
       "3                        Virat Kohli      NaN           NaN            NaN   \n",
       "4                           10 teams      NaN           NaN            NaN   \n",
       "..                               ...      ...           ...            ...   \n",
       "94                                 8      NaN           NaN            NaN   \n",
       "95             1920 x 1080 = 2073600      NaN           NaN            NaN   \n",
       "96                        Jimmy Page      NaN           NaN            NaN   \n",
       "97                                 3      NaN           NaN            NaN   \n",
       "98  A circle does not have any sides      NaN           NaN            NaN   \n",
       "\n",
       "    Sequential_online  Sequential_online_time  Sequential_online_error  \\\n",
       "0                 1.0                    11.0                      NaN   \n",
       "1                 1.0                    65.0                      1.0   \n",
       "2                 0.0                    45.0                      NaN   \n",
       "3                 0.0                    24.0                      NaN   \n",
       "4                 1.0                    64.0                      NaN   \n",
       "..                ...                     ...                      ...   \n",
       "94                NaN                     NaN                      NaN   \n",
       "95                NaN                     NaN                      NaN   \n",
       "96                NaN                     NaN                      NaN   \n",
       "97                NaN                     NaN                      NaN   \n",
       "98                NaN                     NaN                      NaN   \n",
       "\n",
       "    Merged_online  Merged_online_time  Merged_online_error  \n",
       "0             1.0                37.0                  1.0  \n",
       "1             1.0                58.0                  NaN  \n",
       "2             0.0                35.0                  NaN  \n",
       "3             1.0                37.0                  NaN  \n",
       "4             0.0                36.0                  NaN  \n",
       "..            ...                 ...                  ...  \n",
       "94            NaN                 NaN                  NaN  \n",
       "95            NaN                 NaN                  NaN  \n",
       "96            NaN                 NaN                  NaN  \n",
       "97            NaN                 NaN                  NaN  \n",
       "98            NaN                 NaN                  NaN  \n",
       "\n",
       "[99 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Question  : who got Nobel Prize 2023 in Peace ?\n",
      "0 Actual Ans: Narges Mohammadi\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search this on Google to find the most up-to-date information.\n",
      "Action: Google Search\n",
      "Action Input: \"Nobel Prize 2023 Peace winner\"\u001b[0m\u001b[36;1m\u001b[1;3mNarges Mohammadi\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Narges Mohammadi\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'llm_generated': 'Narges Mohammadi', 'actual_answer': 'Narges Mohammadi', 'text': 'Yes, the answer is available in the Prompt1.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "POSITIVE\n",
      "{'label': 'POSITIVE', 'score': 0.5993630886077881}\n",
      "TRUE\n",
      "Data saved\n",
      "1 Question  : who got The Nobel Prize in Physiology or Medicine 2023 ?\n",
      "1 Actual Ans: Drew Weissman & Katalin Karikó\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Google Search to find out who won the Nobel Prize in Physiology or Medicine in 2023.\n",
      "Action: Google Search\n",
      "Action Input: Nobel Prize in Physiology or Medicine 2023 winner\u001b[0m\u001b[36;1m\u001b[1;3mThe Nobel Prize in Physiology or Medicine 2023 · Katalin Karikó · Drew Weissman. The Nobel Prize in Physiology or Medicine 2023 · Katalin Karikó · Drew Weissman ... The Nobel Assembly at the Karolinska Institutet has decided to award the 2023 Nobel Prize in Physiology or Medicine jointly to Katalin Karikó and Drew Weissman ... The University of Pennsylvania messenger RNA pioneers have been named winners of the 2023 Nobel Prize in Physiology or Medicine. The 2023 Nobel prize in Physiology or Medicine has been awarded to Katalin Karikó and Drew Weissman for their “discoveries concerning ... Katalin Karikó and Drew Weissman were jointly awarded the 2023 Nobel Prize in Physiology or Medicine for their discoveries concerning ... The Nobel Prize in Physiology or Medicine 2023 was awarded jointly to Katalin Karikó and Drew Weissman “for their discoveries concerning nucleoside base ... As of 2023, 115 Nobel Prizes in Physiology or Medicine have been awarded to 227 laureates, 214 men and 13 women. The first one was awarded in 1901 to the German ... Nobel Prize lecture: Katalin Karikó, Nobel Prize in Physiology or Medicine 2023 ... Duration: 42:17. Posted: Dec 26, 2023. Drew Weissman has won the Nobel Prize in Physiology or Medicine, along with collaborator Katalin Karikó. ... Immunologist Drew Weissman keynoted ...\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Katalin Karikó and Drew Weissman won The Nobel Prize in Physiology or Medicine 2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'llm_generated': 'Katalin Karikó and Drew Weissman won The Nobel Prize in Physiology or Medicine 2023.', 'actual_answer': 'Drew Weissman & Katalin Karikó', 'text': 'Yes, the answer is available in the Prompt1.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "POSITIVE\n",
      "{'label': 'POSITIVE', 'score': 0.5993630886077881}\n",
      "TRUE\n",
      "Data saved\n",
      "2 Question  : who won 2023 Cricket World Cup?\n",
      "2 Actual Ans: Australia\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Google Search to find the answer to this question.\n",
      "Action: Google Search\n",
      "Action Input: 2023 Cricket World Cup winner\u001b[0m\u001b[36;1m\u001b[1;3mAustralian Men’s Cricket Team\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Australian Men’s Cricket Team\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'llm_generated': 'Australian Men’s Cricket Team', 'actual_answer': 'Australia', 'text': \"Yes, the answer is available in the Prompt1. The Australian Men's Cricket Team is from Australia.\"}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "NEGATIVE\n",
      "{'label': 'NEGATIVE', 'score': 0.8631630539894104}\n",
      "FALSE\n",
      "Data saved\n",
      "3 Question  : Who is the player of the series in 2023 Cricket World Cup?\n",
      "3 Actual Ans: Virat Kohli\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for the player of the series in the 2023 Cricket World Cup.\n",
      "Action: Google Search\n",
      "Action Input: Player of the series in 2023 Cricket World Cup\u001b[0m\u001b[36;1m\u001b[1;3mVirat Kohli was the player of the tournament and also scored the most runs; Mohammed Shami was the leading wicket-taker. A total of 1,250,307 spectators ... Discover the top international players and winners of awards with International Cricket Council. Virat Kohli, Rohit Sharma, Jasprit Bumrah, and Mohammed Shami were among the frontrunners for the ICC World Cup 2023 POTT award. Bet here on ... 2023 World Cup Player of the Tournament Contenders · Virat Kohli (India) · Mohammed Shami (India) · Rohit Sharma (India) · Jasprit Bumrah (India) ... Indian star batter Virat Kohli has been awarded with the \"player of the tournament\" title in the ICC World Cup 2023 for his exemplary ... Official Cricket World Cup website - live matches, scores, news, highlights, commentary, rankings, videos and fixtures from the International Cricket ... Player of the tournament at the ICC Men's Cricket World Cup 2023, Virat Kohli was announced as the ICC Men's ODI Cricketer of the Year 2023. The oldest player of the tournament was Dutch player Wesley Barresi, who was 39 years old, while the youngest was Afghan spinner Noor Ahmad, who was 18. Virat Kohli has been named player of the tournament at ICC Men's Cricket World Cup 2023 ... Duration: 3:16. Posted: Nov 19, 2023. Pat Cummins was the captain throughout it all and will go down as one of Australia's great all-format cricket leaders. Travis Head, meanwhile, ...\u001b[0m\u001b[32;1m\u001b[1;3mI now know the player of the series in the 2023 Cricket World Cup.\n",
      "Final Answer: Virat Kohli\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'llm_generated': 'Virat Kohli', 'actual_answer': 'Virat Kohli', 'text': 'Yes, the answer is available in the Prompt1.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "POSITIVE\n",
      "{'label': 'POSITIVE', 'score': 0.5993630886077881}\n",
      "TRUE\n",
      "Data saved\n",
      "4 Question  : How many teams are their in 2023 Cricket World Cup?\n",
      "4 Actual Ans: 10 teams\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search this on Google to find the most up-to-date information.\n",
      "Action: Google Search\n",
      "Action Input: \"2023 Cricket World Cup teams\"\u001b[0m\u001b[36;1m\u001b[1;3mThe tournament was contested by ten national teams, maintaining the same format used in 2019. In the knockout stage, India and Australia beat New Zealand and ... Afghanistan ; Australia ; Bangladesh ; England ; India. Official Cricket World Cup website - live matches, scores, news, highlights, commentary, rankings, videos and fixtures from the International Cricket ... Latest Highlights ; Australia. 117 ; India. 117 ; England. 115 ; New Zealand. 101 ; South Africa. 99. 2023 Cricket World Cup squads · 1 Key · 2 Afghanistan · 3 Australia · 4 Bangladesh · 5 England · 6 India · 7 Netherlands · 8 New Zealand ... Rohit Sharma (India). · Quinton de Kock (wk) (South Africa). · Rachin Ravindra (New Zealand). · Virat Kohli (India). · Daryl Mitchell (New Zealand). Hosts India have topped the standings and will play fourth-placed New Zealand in the first semifinal. Second-place South Africa take on third- ... The matches will be played at 10 venues across India, including Ahmedabad, Chennai, Delhi, Dharamsala, Hyderabad, Kolkata, Lucknow, Mumbai, and Pune. The final ... India will play their next five matches against Bangladesh, New Zealand, England, Sri Lanka and South Africa respectively before playing their final league- ... When are the 2023 Cricket World Cup semi-finals? ... The first semi-final between India and New Zealand is on Wednesday, November 15, starting at ...\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: There are ten teams in the 2023 Cricket World Cup.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'llm_generated': 'There are ten teams in the 2023 Cricket World Cup.', 'actual_answer': '10 teams', 'text': 'Yes, the answer is available in the Prompt1.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "POSITIVE\n",
      "{'label': 'POSITIVE', 'score': 0.5993630886077881}\n",
      "TRUE\n",
      "Data saved\n",
      "5 Question  : Bangladesh won how many matches in 2023 Cricket World Cup Mens ODI?\n",
      "5 Actual Ans: 2 matches\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search this on Google to find the most up-to-date information.\n",
      "Action: Google Search\n",
      "Action Input: \"Bangladesh 2023 Cricket World Cup Mens ODI matches won\"\u001b[0m\u001b[36;1m\u001b[1;3mBangladesh won the toss and elected to bat. The match was reduced to 37 ... ^ \"ICC World Cup 2023: All the squads for ICC Men's Cricket World Cup 2023\". ICC ... Bangladesh won by 22 runs​​ But in their very next match they stunned the cricket world by defeating Pakistan, the Champion of 1992 Cricket World Cup by 62 runs ... Bronze medal match - Bangladesh won by 6 wickets (with 0 balls remaining) (DLS method). RESULT. 3rd Match • Dharamsala, October 07, 2023, ICC Cricket World ... View matches, standings, news, videos, statistics and teams for ICC Men's ODI World Cup 2023. ENG vs BAN ; Player Of The Match. England. Dawid Malan ; Match number, ODI no. 4664 ; Hours of play (local time), 10.30 start, First Session 10.30-14.00, Interval ... India won by 243 runs. Mon, 06 Nov 2023. 38th Match • Delhi, Arun Jaitley Stadium. sri-lanka. SL. 279 (49.3). bangladesh. BAN. 282-7 (41.1). Bangladesh won by 3 ... The Netherlands recorded it biggest victory in ODI World Cup history after defeating Bangladesh by 87 runs at the Eden Gardens in Kolkata on ... The toss was won by New Zealand, who chose to bowl first. Bangladesh scored 245 runs in 50 overs while losing 9 wickets. Official Cricket World Cup website - live matches, scores, news, highlights, commentary, rankings, videos and fixtures from the International Cricket ... Missing: won | Show results with:won. Bangladesh have won the toss and opted to bowl first. 9:35 am: Head-to-head. The two teams have met only twice in the ODI World Cup with ...\u001b[0m\u001b[32;1m\u001b[1;3mI should refine my search to get the specific number of matches Bangladesh won in the 2023 Cricket World Cup Mens ODI.\n",
      "Action: Google Search\n",
      "Action Input: \"Bangladesh 2023 Cricket World Cup Mens ODI matches won\"\u001b[0m\u001b[36;1m\u001b[1;3mBangladesh won the toss and elected to bat. The match was reduced to 37 ... ^ \"ICC World Cup 2023: All the squads for ICC Men's Cricket World Cup 2023\". ICC ... Bangladesh won by 22 runs​​ But in their very next match they stunned the cricket world by defeating Pakistan, the Champion of 1992 Cricket World Cup by 62 runs ... Bronze medal match - Bangladesh won by 6 wickets (with 0 balls remaining) (DLS method). RESULT. 3rd Match • Dharamsala, October 07, 2023, ICC Cricket World ... View matches, standings, news, videos, statistics and teams for ICC Men's ODI World Cup 2023. India won by 243 runs. Mon, 06 Nov 2023. 38th Match • Delhi, Arun Jaitley Stadium. sri-lanka. SL. 279 (49.3). bangladesh. BAN. 282-7 (41.1). Bangladesh won by 3 ... Get cricket scorecard of 11th Match, BAN vs NZ, ICC Cricket World Cup 2023/24 at MA Chidambaram Stadium, Chepauk, Chennai dated October 13, 2023. The toss was won by New Zealand, who chose to bowl first. Bangladesh scored 245 runs in 50 overs while losing 9 wickets. Official Cricket World Cup website - live matches, scores, news, highlights, commentary, rankings, videos and fixtures from the International Cricket ... Missing: won | Show results with:won. Bangladesh have won the toss and opted to bowl first. 9:35 am: Head-to-head. The two teams have met only twice in the ODI World Cup with ... The Netherlands recorded it biggest victory in ODI World Cup history after defeating Bangladesh by 87 runs at the Eden Gardens in Kolkata on ...\u001b[0m\u001b[32;1m\u001b[1;3mI should try a different search engine to see if I can find the specific number of matches Bangladesh won in the 2023 Cricket World Cup Mens ODI.\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"Bangladesh 2023 Cricket World Cup Mens ODI matches won\"\u001b[0m\u001b[33;1m\u001b[1;3mContinuing their dominant streak in the ICC men's ODI World Cup 2023, Team India steamrolled past Bangladesh in Match 17 of the ongoing tournament. The fixture was played at the MCA Stadium in ... Bit of condemnation of a lacklustre World Cup to date really.\" At least we're spared the premature Best Eva pieces that usually crop up after four days of the football World Cup. 19 Oct 2023 ... Bangladesh won only two matches and finished eighth on the points table. By Koustav Sengupta. ... Going into the ODI World Cup 2023, the Bangladesh Cricket Board (BCB) made it clear that the team ... On form so far, we look to be heading for another convincing win today too, in Pune's first match of the tournament. India have thus far looked the most convincing World Cup favourites since ... ICC Men's World Cup 2023, Dharamsala. Afghanistan 156 (37.2 overs): Gurbaz 47 (62); Mehidy 3-25, Shakib 3-30. Bangladesh 158-4 (34.4 overs): Shanto 59* (83), Mehidy 57 (73) Bangladesh won by six ...\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Bangladesh won two matches in the 2023 Cricket World Cup Mens ODI.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'llm_generated': 'Bangladesh won two matches in the 2023 Cricket World Cup Mens ODI.', 'actual_answer': '2 matches', 'text': 'Yes, the answer is available in the Prompt1.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "POSITIVE\n",
      "{'label': 'POSITIVE', 'score': 0.5993630886077881}\n",
      "TRUE\n",
      "Data saved\n",
      "6 Question  : Which Bangladeshi teenager nominated for International Children's Peace Prize 2023?\n",
      "6 Actual Ans: Mainul Islam\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search this on Google to find the most recent information.\n",
      "Action: Google Search\n",
      "Action Input: Bangladeshi teenager nominated for International Children's Peace Prize 2023\u001b[0m\u001b[36;1m\u001b[1;3mBangladeshi teenager Mainul Islam has received a nomination for the prestigious International Children's Peace Prize 2023, often referred to as the Nobel Prize for children. This notable recognition comes from the Kids Rights Foundation of the Netherlands.\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Bangladeshi teenager Mainul Islam has been nominated for the International Children's Peace Prize 2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'llm_generated': \"Bangladeshi teenager Mainul Islam has been nominated for the International Children's Peace Prize 2023.\", 'actual_answer': 'Mainul Islam', 'text': 'Yes, the answer is available in the Prompt1.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "POSITIVE\n",
      "{'label': 'POSITIVE', 'score': 0.5993630886077881}\n",
      "TRUE\n",
      "Data saved\n",
      "7 Question  : Who was the interim CEO to lead OpenAI in 2023 when Sam Altman was fierd?\n",
      "7 Actual Ans: Mira Murati\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use Google Search to find the answer to this question.\n",
      "Action: Google Search\n",
      "Action Input: Interim CEO of OpenAI 2023\u001b[0m\u001b[36;1m\u001b[1;3mChief technology officer Mira Murati appointed interim CEO to lead OpenAI; Sam Altman departs the company. Search process underway to ... ChatGPT-maker OpenAI has named ex-Twitch boss Emmett Shear as its interim chief executive officer after the startup ousted former head Sam ... Twitch co-founder Emmett Shear became OpenAI's new interim CEO on Monday, the ChatGPT parent's third leader in as many days after Sam Altman ... Sam Altman isn't coming back to OpenAI · In a twist that no one saw coming, Twitch co-founder Emmett Shear is the new CEO. · Share this story. When Emmett Shear, the former chief executive of the livestreaming site Twitch, was named the interim chief executive of OpenAI on Sunday ... OpenAI is bringing in the former head of Twitch as interim CEO just days after the company pushed out its well-known leader, Sam Altman. After a weekend of rumor and speculation, Emmett Shear — former CEO of Twitch — confirmed he will take the top job at OpenAI. Mira Murati, who's taking over as CEO of OpenAI temporarily as Sam Altman departs, comes from a tech background. Mira Murati was elevated from OpenAI's CTO to its interim leader after CEO Sam Altman was ousted Friday. Like all the company's leaders she ... Former Twitch CEO Emmett Shear was appointed as OpenAI's interim CEO on Nov. 19 after Sam Altman was ousted by the board.\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: Mira Murati was the interim CEO to lead OpenAI in 2023 when Sam Altman was fired.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'llm_generated': 'Mira Murati was the interim CEO to lead OpenAI in 2023 when Sam Altman was fired.', 'actual_answer': 'Mira Murati', 'text': 'Yes, the answer is available in the Prompt1.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "POSITIVE\n",
      "{'label': 'POSITIVE', 'score': 0.5993630886077881}\n",
      "TRUE\n",
      "Data saved\n",
      "8 Question  : When the Khawaja Tower in Dhaka's Mohakhali Fire brokout in 2023?\n",
      "8 Actual Ans: Evening\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use Google Search to find information about this specific event.\n",
      "Action: Google Search\n",
      "Action Input: Khawaja Tower Dhaka Mohakhali Fire 2023\u001b[0m\u001b[36;1m\u001b[1;3mKhawaja Tower: Corporate office in Dhaka, Bangladesh. Khawaja Tower Address: 95 গুলশান বাড্ডা সংযোগ সড়ক, Dhaka 1212, Bangladesh. Khawaja Tower Hours: Closed ⋅ Opens 9 AM Sun. Emergency crews are continuing their fire response operation at Khawaja Tower in Dhaka's Mohakhali area as of early Oct. 27. Mohakhali fire -Body 1 (1) People gather near Khawaja Tower in Mohakhali, Dhaka where a fire broke out on Thursday, October 26, 2023. Photo ... Fire service officials working to douse the fire on the 13th floor of the Khawaja Tower in Mohakhali, Dhaka on 26 October 2023. Khawaja Tower in Dhaka's Mohakhali, where a deadly blaze occurred on Oct 26, has caught fire again. The fire was doused quickly after it ... The fire broke out at 16:58 October 26, on the 13th floor of the 14-story Khawaja Tower in the Mohakali district of the Bangladesh capital. The ... Fire service officials working to douse the fire on the 13th floor of the Khawaja Tower in Mohakhali, Dhaka on 26 October 2023. All data centres are down after a massive fire broke out on the 13th floor of Khawaja Tower in Mohakhali, leading to mobile phone customers ... At least three were killed and 10 others injured in a fire that broke out at Khawaja Tower in Dhaka's Mohakhali area this evening. ... Khawaja Tower with ropes in their desperate bid to escape a massive fire that broke out on the 13th floor of the building in Dhaka's Mohakhali. The fire on the 14-story Khawaja Tower in Mohakhali area claimed the life of ... Khawaja Tower in Mohakhali, Dhaka on Thursday, October 26, 2023.\u001b[0m\u001b[32;1m\u001b[1;3mI now know the exact date and time of the fire at Khawaja Tower in Dhaka's Mohakhali in 2023.\n",
      "Final Answer: The fire broke out on October 26, 2023, at 16:58 on the 13th floor of the Khawaja Tower in Dhaka's Mohakhali.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'llm_generated': \"The fire broke out on October 26, 2023, at 16:58 on the 13th floor of the Khawaja Tower in Dhaka's Mohakhali.\", 'actual_answer': 'Evening', 'text': 'Yes, the answer is available in the Prompt1. The fire broke out in the evening on October 26, 2023.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "NEGATIVE\n",
      "{'label': 'NEGATIVE', 'score': 0.9741762280464172}\n",
      "FALSE\n",
      "Data saved\n",
      "9 Question  : When was Dhaka Metro Rail inaugurated in Bangladesh ?\n",
      "9 Actual Ans: 29th  December 2022\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use Google Search to find the answer to this question.\n",
      "Action: Google Search\n",
      "Action Input: \"Dhaka Metro Rail inauguration date in Bangladesh\"\u001b[0m\u001b[36;1m\u001b[1;3mThe first section of MRT Line 6 commenced commercial operations on 29 December 2022, and the second section on 5 November 2023. The other lines are either in planning or under construction. The third phase, which will have an interchange with Line 1, Line 2 and Line 4, is set to be completed by 2026.\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The Dhaka Metro Rail was inaugurated in Bangladesh on 29 December 2022.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-EJ0AHMnsJ140LwlMH1R1tC8e on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "{'llm_generated': \"The fire broke out on October 26, 2023, at 16:58 on the 13th floor of the Khawaja Tower in Dhaka's Mohakhali.\", 'actual_answer': 'Evening', 'text': 'Yes, the answer is available in the Prompt1. The fire broke out in the evening on October 26, 2023.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "NEGATIVE\n",
      "{'label': 'NEGATIVE', 'score': 0.9741762280464172}\n",
      "FALSE\n",
      "Data saved\n",
      "10 Question  : What is the slogan of metro rail in Bangladesh?\n",
      "10 Actual Ans: Moving People Saving Time and Environment\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-EJ0AHMnsJ140LwlMH1R1tC8e on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-EJ0AHMnsJ140LwlMH1R1tC8e on requests per day (RPD): Limit 200, Used 200, Requested 1. Please try again in 7m12s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "{'llm_generated': \"The fire broke out on October 26, 2023, at 16:58 on the 13th floor of the Khawaja Tower in Dhaka's Mohakhali.\", 'actual_answer': 'Evening', 'text': 'Yes, the answer is available in the Prompt1. The fire broke out in the evening on October 26, 2023.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "NEGATIVE\n",
      "{'label': 'NEGATIVE', 'score': 0.9741762280464172}\n",
      "FALSE\n",
      "Data saved\n",
      "11 Question  : Is metro rail off on Friday in bangladesh?\n",
      "11 Actual Ans: Yes\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-EJ0AHMnsJ140LwlMH1R1tC8e on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-EJ0AHMnsJ140LwlMH1R1tC8e on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "{'llm_generated': \"The fire broke out on October 26, 2023, at 16:58 on the 13th floor of the Khawaja Tower in Dhaka's Mohakhali.\", 'actual_answer': 'Evening', 'text': 'Yes, the answer is available in the Prompt1. The fire broke out in the evening on October 26, 2023.'}\n",
      "\n",
      "-----------------------sentiment------------------------------------\n",
      "NEGATIVE\n",
      "{'label': 'NEGATIVE', 'score': 0.9741762280464172}\n",
      "FALSE\n",
      "Data saved\n",
      "12 Question  : Who inaugurated metro rail in bangladesh?\n",
      "12 Actual Ans: Prime Minister Sheikh Hasina\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/openai/_base_client.py:939\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/httpx/_models.py:759\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    758\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 759\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39mtools,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,handle_parsing_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     llm_generated \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain/agents/agent.py:1391\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1391\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1400\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1401\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain/agents/agent.py:1097\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1090\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1095\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1097\u001b[0m         [\n\u001b[1;32m   1098\u001b[0m             a\n\u001b[1;32m   1099\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1100\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1101\u001b[0m                 color_mapping,\n\u001b[1;32m   1102\u001b[0m                 inputs,\n\u001b[1;32m   1103\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1104\u001b[0m                 run_manager,\n\u001b[1;32m   1105\u001b[0m             )\n\u001b[1;32m   1106\u001b[0m         ]\n\u001b[1;32m   1107\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain/agents/agent.py:1097\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1090\u001b[0m     name_to_tool_map: Dict[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1095\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, List[Tuple[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1097\u001b[0m         [\n\u001b[1;32m   1098\u001b[0m             a\n\u001b[1;32m   1099\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_next_step(\n\u001b[1;32m   1100\u001b[0m                 name_to_tool_map,\n\u001b[1;32m   1101\u001b[0m                 color_mapping,\n\u001b[1;32m   1102\u001b[0m                 inputs,\n\u001b[1;32m   1103\u001b[0m                 intermediate_steps,\n\u001b[1;32m   1104\u001b[0m                 run_manager,\n\u001b[1;32m   1105\u001b[0m             )\n\u001b[1;32m   1106\u001b[0m         ]\n\u001b[1;32m   1107\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain/agents/agent.py:1125\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain/agents/agent.py:387\u001b[0m, in \u001b[0;36mRunnableAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Use streaming to make sure that the underlying LLM is invoked in a streaming\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# fashion to make it possible to get access to the individual LLM tokens\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# when using stream_log with the Agent Executor.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Because the response from the plan is not a generator, we need to\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# accumulate the output into final output and return that.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m final_output: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunnable\u001b[38;5;241m.\u001b[39mstream(inputs, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m         final_output \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:2446\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[1;32m   2441\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2442\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   2443\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2444\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2445\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2446\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:2433\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   2428\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2429\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   2430\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2431\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2432\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2433\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   2434\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2435\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[1;32m   2436\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m   2437\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2438\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:1513\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1513\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m   1515\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:2397\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config)\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m   2389\u001b[0m     final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   2390\u001b[0m         final_pipeline,\n\u001b[1;32m   2391\u001b[0m         patch_config(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2394\u001b[0m         ),\n\u001b[1;32m   2395\u001b[0m     )\n\u001b[0;32m-> 2397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_pipeline:\n\u001b[1;32m   2398\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m output\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:1051\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m final: Input\n\u001b[1;32m   1049\u001b[0m got_first_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m got_first_val:\n\u001b[1;32m   1053\u001b[0m         final \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:4173\u001b[0m, in \u001b[0;36mRunnableBindingBase.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[1;32m   4168\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4169\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   4170\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4171\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   4172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 4173\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   4174\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4175\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[1;32m   4176\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[1;32m   4177\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:1061\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         final \u001b[38;5;241m=\u001b[39m final \u001b[38;5;241m+\u001b[39m chunk  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:250\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    244\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[1;32m    245\u001b[0m         e,\n\u001b[1;32m    246\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[1;32m    247\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    248\u001b[0m         ),\n\u001b[1;32m    249\u001b[0m     )\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:234\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    235\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    236\u001b[0m     ):\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:419\u001b[0m, in \u001b[0;36mChatOpenAI._stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m    418\u001b[0m default_chunk_class \u001b[38;5;241m=\u001b[39m AIMessageChunk\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    421\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/openai/_base_client.py:1180\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1168\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1177\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1178\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1179\u001b[0m     )\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/openai/_base_client.py:869\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    862\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    868\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/openai/_base_client.py:945\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    944\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/openai/_base_client.py:991\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m--> 991\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    994\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    995\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    999\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for some warning\n",
    "# pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "start = 0 \n",
    "finish = 100\n",
    "df\n",
    "# print(df.to_string)\n",
    "\n",
    "for i in range(start,finish):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    question = df['Question'][i]\n",
    "    actual_answer = df['Answer'][i]\n",
    "    print(str(i)  +\" Question  : \"+ question)\n",
    "    print(str(i)  +\" Actual Ans: \"+ actual_answer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # llm_chain1 = LLMChain(llm=llm, prompt=prompt1_with_tools)\n",
    "\n",
    "    tool_names = [tool.name for tool in tools]\n",
    "    agent = create_react_agent(\n",
    "\n",
    "        tools=tools ,\n",
    "        prompt=prompt1_with_tools,\n",
    "        llm = llm,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=tools,verbose=True,handle_parsing_errors=True,max_iterations=6)\n",
    "        llm_generated = agent_executor.invoke({\"question\": question})\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        df.loc[i,'Sequential_online_error']=1\n",
    "\n",
    "    # print(\"==========================================================\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "   \n",
    "\n",
    "    try:\n",
    "        llm_chain2 = LLMChain(llm=llm, prompt=prompt2_for_comparison)\n",
    "        final_result = llm_chain2.invoke({\"llm_generated\":llm_generated['output'] , \"actual_answer\" :actual_answer})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        df.loc[i,'Sequential_online_error']=1\n",
    "\n",
    "\n",
    "    print(final_result)\n",
    "    classifier = pipeline(\"sentiment-analysis\" , model = model,tokenizer=tokenizer)\n",
    "    print(\"\\n-----------------------sentiment------------------------------------\")\n",
    "    label = classifier([str(final_result['text'])])\n",
    "    print(label[0][\"label\"])\n",
    "    print(label[0])\n",
    "\n",
    "\n",
    "    # label[0][\"label\"]\n",
    "\n",
    "\n",
    "    if(label[0][\"label\"] == \"POSITIVE\"):\n",
    "        df.loc[i,'Sequential_online']= 1\n",
    "        print(\"TRUE\")\n",
    "    else:\n",
    "        df.loc[i,'Sequential_online']= 0\n",
    "        print(\"FALSE\")\n",
    "\n",
    "\n",
    "    df.loc[i,'Sequential_online_time'] = int(end_time-start_time)\n",
    "    df.to_csv(data_path ,index = False)\n",
    "    print(\"Data saved\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Process finished\")\n",
    "df = pd.read_csv(data_path , index = False)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
