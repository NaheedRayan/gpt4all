{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    \"model\": \"tinyllama:chat\",\n",
    "    \"prompt\": \"[INST] why is the sky blue? [/INST]\",\n",
    "    \"raw\": True,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\")\n",
    "print(response.json())\n",
    "# print(response.text)\n",
    "\n",
    "# formatted_result = json.dumps(response.json(), indent=2)\n",
    "# # print(formatted_result)\n",
    "# print(formatted_result[response])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "llm = ChatOllama(model=\"tinyllama:chat\")\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "# learn more about the LCEL on\n",
    "# https://python.langchain.com/docs/expression_language/why\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# for brevity, response is printed in terminal\n",
    "# You can use LangServe to deploy your application for\n",
    "# production\n",
    "print(chain.invoke({\"topic\": \"Space travel\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Got invalid return object. Expected key `component_name` to be present, but got {'componenent_name': 'Lamp 5', 'reasoning': 'We have to turn it down.', 'compoben_status': 'Off'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTurn down the l5\u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat_instructions\u001b[39m\u001b[38;5;124m'\u001b[39m:format_instructions })\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m response_as_dict \u001b[38;5;241m=\u001b[39m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_as_dict)\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain/output_parsers/structured.py:97\u001b[0m, in \u001b[0;36mStructuredOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     96\u001b[0m     expected_keys \u001b[38;5;241m=\u001b[39m [rs\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m rs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_schemas]\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_and_check_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/gpt4all/env/lib/python3.10/site-packages/langchain_core/output_parsers/json.py:180\u001b[0m, in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m expected_keys:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_obj:\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m    181\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid return object. Expected key `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto be present, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m         )\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_obj\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Got invalid return object. Expected key `component_name` to be present, but got {'componenent_name': 'Lamp 5', 'reasoning': 'We have to turn it down.', 'compoben_status': 'Off'}"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# https://colab.research.google.com/drive/1D3i-4yiPvRmUX7PWWiat7iNX-HoggKS9?usp=sharing#scrollTo=zOfAmQ7LwpJC\n",
    "\n",
    "# llm = Ollama(model=\"tinyllama:chat\")\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "component_name_schema = ResponseSchema(name=\"component_name\",\n",
    "                             description=\"This is the name of the component or device\")\n",
    "\n",
    "reasoning_schema = ResponseSchema(name=\"reasoning\",\n",
    "                                      description=\"just reasoning\")\n",
    "\n",
    "component_status_schema = ResponseSchema(name=\"component_status\",\n",
    "                                    description=\"On or off\")\n",
    "\n",
    "response_schemas = [component_name_schema, \n",
    "                    reasoning_schema,\n",
    "                    component_status_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "\n",
    "Select the tools as best as you can. You can also ignore if not possible. \n",
    "\n",
    "example : \n",
    "                                      \n",
    "Question: Switch On the light1\n",
    "component_name: light1\n",
    "reasoning: we have to switch it on\n",
    "component_status: on                                  \n",
    "\n",
    "Question: Switch off the light1\n",
    "component_name: light1\n",
    "reasoning: we have to switch it off\n",
    "component_status: off                                   \n",
    "\n",
    "                              \n",
    "\n",
    "\n",
    "Read the Question1 and answer using the following format for the output:\n",
    "{format_instructions}                                                         \n",
    "\n",
    "                                      \n",
    "Begin!\n",
    "        \n",
    "Question1 : {input}\n",
    "                             \n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm \n",
    "\n",
    "\n",
    "\n",
    "response = chain.invoke({'input':\"Turn down the l5\" , 'format_instructions':format_instructions })\n",
    "\n",
    "# print(data)\n",
    "\n",
    "response_as_dict = output_parser.parse(response)\n",
    "print(response_as_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
